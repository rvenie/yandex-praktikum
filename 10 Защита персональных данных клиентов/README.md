# Защита персональных данных клиентов
## Краткое описание
Необходимо защитить данные клиентов страховой компании. Для этого нужно разработать такой метод преобразования данных, 
чтобы по ним было сложно восстановить персональную информацию, и обосновать корректность его работы.

Нужно защитить данные так, чтобы при преобразовании качество моделей машинного обучения не ухудшилось. Подбирать наилучшую модель не требуется.

В проекте предлагается ответить на вопрос *изменится ли качество линейной регрессии, если признаки умножить на обратимую матрицу?*

Для этого необходимо сначала показать что результат регрессии не исзменится с помощью математического вывода, затем оценть качество 
модели с данным преобразованием и без него средствами **sklearn**.

**Основная задача проекта освежить знания линейной алгебры.**

## Математический вывод
Ниже продублирован математический вывод, показывающий, что умножение признаков на обратимую матрицу не изменяет
результат линейной регрессии, из [тетрадки](/project10.ipynb).

Формулы записаны в нотации LaTex.


Введём следующие обозначения:

**Матрица признаков**

$$X = \left(\begin{array}{c}
           1   & x_{01} & x_{02} & ... & x_{0n} \\ 
           1   & x_{11} & x_{12} & ... & x_{1n} \\
           ... & ...    & ...    & ... & ...    \\
           1   & x_{m1} & x_{m2} & ... & x_{mn}
\end{array}\right)$$

**Вектор целевого признака**

$$y = \left(\begin{array}{c} y_{0} \\ y_{1} \\ ... \\ y_{m} \end{array}\right)$$

**Обратиая матрица, на которую умножаются признаки**

$$P = \left(\begin{array}{c}
           1   & p_{01} & p_{02} & ... & p_{0n} \\ 
           1   & p_{11} & p_{12} & ... & p_{1n} \\
           ... & ...    & ...    & ... & ...    \\
           1   & p_{n1} & p_{n2} & ... & p_{nn}
\end{array}\right)$$

**Вектор весов линейной регрессии**

$$w = \left(\begin{array}{c} w_{0} \\ w_{1} \\ ... \\ w_{n} \end{array}\right)$$

Предсказания модели линейной регрессии вычисляют по следующей формуле:

$$\hat{y} = Xw$$

Задача обучения формулируется следующим образом:

$$ w = \arg\min_w MSE(\hat{y}, y) $$

$$ w = \arg\min_w MSE(Xw, y) ,$$ 

тогда формула обучения имеет вид выражения

$$ w = (X^T X)^{-1} X^T y $$

Предположим, что, если умножить признаки $X$ на обратимую матрицу $P$, то предсказания модели будут иметь следующий вид: 

$$ \hat{y}^\star = (XP)w^\star , где $$

$w^\star$ — вектор весов линейной регрессии, выполненной после перемножения признаков $X$ на обратимую матрицу $P$.

Чтобы доказать, что умножение $X$ на $P$ не изменяет качество линейной регрессии, покажем, что $\hat{y}=\hat{y}^\star$.

Cформулируем задачу обучения для $w^\star$

$$ w^\star = \arg\min_{w^\star} MSE(\hat{y}^\star, y) $$

$$ w^\star = \arg\min_{w^\star} MSE((XP)w^\star, y) $$

Формула обучения, если вместо $X$ подставить $XP$, а вместо $w$ подставить $w^\star$ примет следующий вид:

$$ w^\star = ((XP)^T (XP))^{-1} (XP)^T y $$


Воспользуемся свойством ассоциативности умножения матриц и перегруппируем скобки под знаком возведения в степень $-1$:

$$ w^\star = \left(\left((XP)^T X\right) P\right)^{-1} (XP)^T y $$

Раскроем внешние скобки:

$$ w^\star = P^{-1}\left((XP)^T X\right)^{-1} P^T X^T y $$

Раскроем скобку $(XP)^T$:

$$ w^\star = P^{-1}\left(P^T X^T X\right)^{-1} P^T X^T y $$

Сгруппируем $X^T X$ и раскроем скобку $\left(P^T (X^T X)\right)^{-1}$:

$$ w^\star = P^{-1}\left(X^T X\right)^{-1} \left(P^T\right)^{-1} P^T X^T y $$

Сократим $\left(P^T\right)^{-1}$ и $P^T$:

$$ w^\star = P^{-1}\left(X^T X\right)^{-1} X^T y $$

В правой части равенства выполним замену $(X^T X)^{-1} X^T y$ на $w$:

$$ w^\star = P^{-1} w $$

**Мы нашли связь между параметрами линейной регресси исходной задачи и преобразованной $w^\star = P^{-1} w$**.


Подставим полученное выражение в формулу предсказаний линейной регрессии преобразованной задачи 
$\hat{y}^\star = (XP)w^\star$:

$$\hat{y}^\star = (XP)P^{-1}w$$

Уберём скобки и сократим $P$:

$$\hat{y}^\star = Xw$$

**Таким образом, мы получили $\hat{y}^\star = \hat{y}$, что и требовалось доказать.**

## Общий вывод

Защита данных не ухудшила показатель качества модели.
Рукописная модель не уступает в качестве таковой из пакета sklearn
